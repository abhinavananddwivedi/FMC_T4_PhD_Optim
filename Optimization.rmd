---
title: "Optimization"
author: Abhinav Anand, IIMB
date: '`r format(Sys.time(), "%Y/%m/%d")`' #current date

output:
  pdf_document:
    keep_tex: true

fontsize: 11pt
documentclass: article
geometry: margin = 1.5in

linkcolor: blue
urlcolor: red
citecolor: magenta

citation_package: natbib
bibliography: Working_Paper.bib

header-includes:
   - \linespread{1.25}
   - \usepackage{amsmath}


---

```{r setup, eval=T, message=FALSE, warning=T, include=FALSE}

library(tidyverse)
library(rmarkdown)
library(knitr)
library(xml2)

knitr::opts_chunk$set(echo = T, 
                      warning = T, 
                      message = F, 
                      eval = T, 
                      include = T,
                      fig.height=4, 
                      fig.width=4
                      )
```

# Background

Problems in finance and economics are often concerned with the behavior of agents who are considered to be utility maximizers. Utility functions are thought to be monotonic in their variables---more of a utility enhancing variable is better than less---and are thought to obey diminishing returns as the variables scale.[^note_utility] Additionally, real-life constraints ensure that variables are bounded. Hence optimization of functions under constraints forms an important discipline in the study of such subjects.

[^note_utility]: This utility function can assume a variety of forms. For example, if the 'agent' is a firm, its utility is its profit function; if its a government, it could be some (aggregate) social welfare function etc.

After linear optimization, quadratic optimization problems are the simplest since they can be captured in *quadratic forms* and can be represented via symmetric matrices with special properties. A distinguishing feature of quadratic optimization is the presence of first and second order conditions that characterize the nature of the extreme point.

## Quadratic Optimization

In one dimension, $x\in \mathbb{R}$ the simplest quadratic objective functions can be $f(x)=\{x^2, -x^2\}$ with the first attaining a global minimum and the second a global maximum at $x=0$.

```{r opt_dim_1, cache=TRUE}
x <- -500:500
y_1 <- x^2
y_2 <- -x^2
data_l <- cbind(x, y_1, y_2) %>% 
  dplyr::as_tibble() %>% #wide format
  tidyr::gather(., 
                y_1:y_2, 
                key = 'y', 
                value = 'f') #long format

ggplot(data_l, aes(x, f, color = y)) +
  geom_line() +
  geom_hline(yintercept = 0) +
  theme_minimal()

```

### Quadratic Forms

For the case when $x = (x_1, x_2) \in \mathbb{R}^2$, a general quadratic form is the following:
\[
Q(x) = a_{11}x_1^2 + a_{22}x_2^2 + 2a_{12}x_1x_2
\]
which may be represented as a matrix:
\[
Q(x) = [x_1 x_2]\begin{bmatrix}a_{11} & a_{12}\\ a_{12} & a_{22}\end{bmatrix}
\begin{bmatrix}x_1 \\ x_2\end{bmatrix}
\]
Similarly, when $x = (x_1, x_2, x_3) \in \mathbb{R}^3$, the form becomes
\[
Q(x) = [x_1 x_2 x_3]\begin{bmatrix}a_{11} & a_{12} & a_{13}\\ a_{12} & a_{22} & a_{23}\\
a_{13} & a_{23} & a_{33}\end{bmatrix} \begin{bmatrix}x_1 \\ x_2 \\ x_3\end{bmatrix}
\]
and in general, when $x = (x_1, \hdots, x_n) \in \mathbb{R}^n$:
\[
Q(x) = [x_1 \hdots x_n]\begin{bmatrix}a_{11} & \hdots & a_{1n}\\ \vdots\\
a_{1n} & \hdots & a_{nn}\end{bmatrix} \begin{bmatrix}x_1\\ \vdots\\ x_n\end{bmatrix}
\]
It's easy to see that $Q(0) = 0$. If $a>0, ax^2>0$ and we can call the quadratic form "positive definite". Similarly, if $a<0, ax^2<0$ and we call the form "negative definite". This carries over when $x\in \mathbb{R}^2$, since $Q(x_1, x_2) = x_1^2+x_2^2>0$ (positive definite) and $Q(x_1, x_2) = -x_1^2-x_2^2>0$ (negative definite). Additionally for the case when $Q(x_1, x_2) = x_1^2-x_2^2>0$, the quadratic form is *indefinite*. 

In general, symmetric matrices are called positive (semi)definite, negative (semi)definite etc. according to the definiteness of the expression $Q(x) = x^{\top}Ax$.[^note_definite]

[^note_definite]: Hence if $\forall x\neq0\in \mathbb{R}^n, x^{\top}Ax\geq 0$ the form is positive semidefinite and so on.

# Unconstrained Optimization

While it's clear what a maximum is in one dimension, what should be its definition in two dimensions and beyond? The central idea remains the same: $x^*$ is the maximum if there is no other $x$ in the domain of $f(x)$ such that $f(x)\geq f(x^*)$. Note that this definition is general and independent of the dimension of the underlying space from where we pick $x$. The only difference in dimensions one and beyond is the structure of the domain; and while in one dimension it must take the form $x_L\leq x\leq x_H$, one needs such conditions for each component when $x\in \mathbb{R}^n: x_{L1}\leq x_1\leq x_{H1}, \hdots, x_{Ln}\leq x_n\leq x_{Hn}$. With this interpretation in mind, we are ready to formulate the necessary and sufficient conditions for the optimal of an unconstrained optimization program.

## First Order Conditions

When the dimension of the domain is 1, $x_L\leq x\leq x_H$, the first order condition stipulates that at the *critical point* the derivative be 0.[^note_FOC_dim_1] Whether this critical point is a (local) maximum or (local) minimum or neither, depends on the *second order condition* on the derivative of the derivative.

The example of $f(x) = x^2$ illustrates this idea.

```{r FOC_dim_1, dependson='opt_dim_1'}

ggplot(data = dplyr::as_tibble(cbind(x, y_1)),
       mapping = aes(x, y_1)) +
  geom_line() +
  theme_minimal()

```

[^note_FOC_dim_1]: Why must this be so? Can you see the case for the plot $f(x) = x^2$ and reason why? 

End points or corner points are generally not considered critical points though they can contain extreme points. (For the plot $f(x)=x^2$ what is/are maximal points if $x\in [l, b]$?) Conventionally, critical points are thought to be points interior to the domain.

Functions can be separated into classes on the basis of how differentiable they are (the differentiability class). For example, all functions that are continuous form the $C^0$ class; those that are differentiable and their derivatives also continuous form the class $C^1$; those whose second derivatives are continuous form the class $C^2$ and so on.[^note_smooth]

**Theorem:** If $x^*\in \mathbb{R}^n$ is an interior point in the domain of the real-valued function $F\in C^1$, then it is a critical point when
\[
  \frac{\partial F}{\partial x_i}(x^*) = 0, i\in \{1, 2,\hdots, n\}
\]

[^note_smooth]: A function is "smooth" if all derivatives are continous: $f\in C^{\infty}$; and analytical $f \in C^{\omega}$ if it is smooth *and* its Taylor approximation converges to its function value at all points in the domain.

## Second Order Conditions

Critical points are interior points at which all first partial derivatives are 0. However, this in itself is not enough for us to know if the critical point is a maximum, a minimum or neither. For this, we need to consider its second derivatives.

For the function $F\in C^2, x\in U\subset \mathbb{R}^n$, ($U$ is open in $\mathbb{R}^n$) the *Hessian* is the following second derivative matrix:
\[
D^2F(x^*) = \begin{bmatrix}
\frac{\partial F}{\partial x_1^2}(x^*) & \hdots &\frac{\partial F}{\partial x_n\partial x_1}(x^*)\\
\vdots & \ddots & \vdots\\
\frac{\partial F}{\partial x_1\partial x_n}(x^*) & \hdots & \frac{\partial F}{\partial x_n^2}(x^*)
\end{bmatrix}
\]

Since for the function $F\in C^2$, the cross-partials $\frac{\partial F}{\partial x_i \partial x_j} = \frac{\partial F}{\partial x_j \partial x_i}$, the Hessian is symmetric.

The second order conditions are based on the Hessian. Just as in the one dimensional case, where there occurs a maximum at a critical point if the second derivative is negative; there occurs a maximum at the critical point $x^*\in U\subset \mathbb{R}^n$ if the Hessian is *negative definite*.

